{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvgr0YneWPHEBDwCsydcSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AncalagonDBlack/source-code-seo-tool/blob/master/Keyword_Clustering_Tool_V3_polyfuzz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0WlhQb517eO",
        "outputId": "0460f481-303a-4edd-ff4c-e9bf88a57cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polyfuzz[fast] in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (4.64.1)\n",
            "Requirement already satisfied: rapidfuzz>=0.13.1 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (2.13.7)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (0.11.2)\n",
            "Requirement already satisfied: sparse-dot-topn>=0.2.9 in /usr/local/lib/python3.8/dist-packages (from polyfuzz[fast]) (0.3.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.25.3->polyfuzz[fast]) (2022.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->polyfuzz[fast]) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.2->polyfuzz[fast]) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (4.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install polyfuzz[fast] #dùng polyfuzz https://maartengr.github.io/PolyFuzz/\n",
        "!pip install chardet\n",
        "import pandas as pd\n",
        "import sys\n",
        "from google.colab import files\n",
        "from polyfuzz import PolyFuzz\n",
        "import chardet\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the parent cluster name using the keyword with the highest search volume (recommended)\n",
        "parent_by_vol = True\n",
        "drop_site_links = False\n",
        "drop_image_links = False\n",
        "sim_match_percent = 1\n",
        "url_filter = \"\"\n",
        "min_volume = 10  # Chỉnh min volume search nên để 10 / impressions to filter on"
      ],
      "metadata": {
        "id": "8K7Vza_84ng_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Custom Intent Classifiers\n",
        "You Can Add Custom Words to Classify Keywords with Below. e.g. 'vs' = Commercial Investigation, 'how' = 'informational' etc."
      ],
      "metadata": {
        "id": "PwqckWqf4tu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info_filter = \"what|where|why|when|who|how|which|tip|guide|tutorial|ideas|example|learn|wiki|in mm|in cm|in ft|in feet\"\n",
        "comm_invest_filter = \"best|vs|list|compare|review|list|top|difference between\"\n",
        "trans_filter = \"purchase|bargain|cheap|deal|value|closeout|buy|shop|price|coupon|discount|price|pricing|delivery|shipping|order|returns|sale|amazon|target|ebay|walmart|cost\""
      ],
      "metadata": {
        "id": "CbNDpGcP4qYG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload từ khóa ở đây.\n"
      ],
      "metadata": {
        "id": "Z0HUh2Ve4v7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the keyword export\n",
        "upload = files.upload()\n",
        "input_file = list(upload.keys())[0]  # get the name of the uploaded file\n",
        "# test the file extension\n",
        "file_extension = os.path.splitext(input_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "xy1vpr4Y4001",
        "outputId": "a56d2262-9f03-40f1-e9d3-04a12c7096d4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-945bede4-d086-4aac-84c5-7f969179adee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-945bede4-d086-4aac-84c5-7f969179adee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Keyword Stats 2023-01-09 at 09_51_39 - Trang tính1 (2).csv to Keyword Stats 2023-01-09 at 09_51_39 - Trang tính1 (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------- auto detect character encoding ----------------------------------------------------\n",
        "with open(input_file, 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(10000))\n",
        "\n",
        "# if the encoding is utf-16 use a space separator, else ','\n",
        "if result['encoding'] == \"UTF-16\":\n",
        "    white_space = True\n",
        "else:\n",
        "    white_space = False\n",
        "\n",
        "if (\n",
        "    file_extension[1] == \".xlsx\"\n",
        "    or file_extension[1] == \".xls\"\n",
        "    or file_extension[1] == \".xlsm\"\n",
        "    or file_extension[1] == \".xlsb\"\n",
        "    or file_extension[1] == \".odf\"\n",
        "    or file_extension[1] == \".ods\"\n",
        "    or file_extension[1] == \".odt\"\n",
        "):\n",
        "    df_1 = pd.read_excel(input_file, engine=\"openpyxl\")\n",
        "else:\n",
        "    try:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=result[\"encoding\"],\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )\n",
        "    # fall back to utf-8\n",
        "    except UnicodeDecodeError:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=\"utf-8\",\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )"
      ],
      "metadata": {
        "id": "7te7_GvJ459d"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- check if single column import / and write header if missing -------------------------------\n",
        "\n",
        "# check the number of columns\n",
        "col_len = len(df_1.columns)\n",
        "col_name = df_1.columns[0]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"Keyword\":\n",
        "    df_1.columns = [\"Keyword\"]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"keyword\":\n",
        "    df_1.columns = [\"Keyword\"]"
      ],
      "metadata": {
        "id": "zdUdRlI346gn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------- detect if import file is adwords and remove the first two rows ----------------------------\n",
        "adwords_check = False\n",
        "if col_name == \"Search terms report\":\n",
        "    df_1.columns = df_1.iloc[1]\n",
        "    df_1 = df_1[1:]\n",
        "    df_1 = df_1.reset_index(drop=True)\n",
        "\n",
        "    new_header = df_1.iloc[0]  # grab the first row for the header\n",
        "    df_1 = df_1[1:]  # take the data less the header row\n",
        "    df_1.columns = new_header  # set the header row as the df header\n",
        "    adwords_check = True"
      ],
      "metadata": {
        "id": "fze_mh3T48G1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------- Check if csv data is gsc and set bool ----------------------------------------------\n",
        "\n",
        "if 'Impressions' in df_1.columns:\n",
        "    gsc_data = True"
      ],
      "metadata": {
        "id": "R-11hHna4-rd"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- standardise the column names between ahrefs v1/v2/semrush/gsc keyword exports ----------------------\n",
        "\n",
        "df_1.rename(\n",
        "    columns={\n",
        "        \"Current position\": \"Position\",\n",
        "        \"Current URL\": \"URL\",\n",
        "        \"Current URL inside\": \"Page URL inside\",\n",
        "        \"Current traffic\": \"Traffic\",\n",
        "        \"KD\": \"Difficulty\",\n",
        "        \"Keyword Difficulty\": \"Difficulty\",\n",
        "        \"Search Volume\": \"Volume\",\n",
        "        \"page\": \"URL\",\n",
        "        \"query\": \"Keyword\",\n",
        "        \"Top queries\": \"Keyword\",\n",
        "        \"Impressions\": \"Volume\",\n",
        "        \"Clicks\": \"Traffic\",\n",
        "        \"Search term\": \"Keyword\",\n",
        "        \"Impr.\": \"Volume\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ],
      "metadata": {
        "id": "KBRKCDX74_cl"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------ check number of imported rows and warn if excessive -----------------------------------\n",
        "\n",
        "row_len = len(df_1)\n",
        "if col_len > 1:\n",
        "    # --------------------------------- clean the data pre-grouping ----------------------------------------------------\n",
        "\n",
        "    if url_filter:\n",
        "        print(\"Processing only URLs containing:\", url_filter)\n",
        "\n",
        "    try:\n",
        "        df_1 = df_1[df_1[\"URL\"].str.contains(url_filter, na=False)]\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "    # ========================= clean strings out of numerical columns (adwords) ========================================\n",
        "\n",
        "    try:\n",
        "        df_1[\"Volume\"] = df_1[\"Volume\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Traffic\"] = df_1[\"Traffic\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Conv. value / click\"] = df_1[\"Conv. value / click\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].replace(\" --\", \"0\", regex=True)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].str.replace(\"\\%\", \"\").astype(float)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].astype(float)\n",
        "        df_1[\"Conversions\"] = df_1[\"Conversions\"].astype(int)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].round(2)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].round(2)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(\"Total: \", na=False)]  # remove totals rows\n",
        "    df_1 = df_1[df_1[\"Keyword\"].notna()]  # keep only rows which are NaN\n",
        "    df_1 = df_1[df_1[\"Volume\"].notna()]  # keep only rows which are NaN\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(str)\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].apply(lambda x: x.replace(\"0-10\", \"0\"))\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(float).astype(int)\n",
        "\n",
        "    # drop sitelinks\n",
        "\n",
        "    if drop_site_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Sitelinks\", na=False)]  # drop sitelinks\n",
        "        except KeyError:\n",
        "            pass\n",
        "        try:\n",
        "            if gsc_data:\n",
        "                df_1 = df_1.sort_values(by=\"Traffic\", ascending=False)\n",
        "                df_1.drop_duplicates(subset=\"Keyword\", keep=\"first\", inplace=True)\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "    if drop_image_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Image pack\", na=False)]  # drop image pack\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    df_1 = df_1[df_1[\"Volume\"] > min_volume]"
      ],
      "metadata": {
        "id": "OVpUtGy15A7V"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------- do the grouping ----------------------------------------------------------------\n",
        "\n",
        "df_1_list = df_1.Keyword.tolist()  # create list from df\n",
        "model = PolyFuzz(\"TF-IDF\")\n",
        "\n",
        "cluster_tags = df_1_list[::]\n",
        "cluster_tags = set(cluster_tags)\n",
        "cluster_tags = list(cluster_tags)\n",
        "\n",
        "try:\n",
        "    model.match(df_1_list, cluster_tags)\n",
        "except ValueError:\n",
        "    print(\"Empty Dataframe, Can't Match - Check the URL Filter!\")\n",
        "    sys.exit()\n",
        "\n",
        "model.group(link_min_similarity=sim_match_percent)\n",
        "df_matched = model.get_matches()\n",
        "     "
      ],
      "metadata": {
        "id": "StnhtmCJ5DI9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------- clean the data post-grouping ---------------------------------------------------------\n",
        "\n",
        "df_matched.rename(columns={\"From\": \"Keyword\", \"Group\": \"Cluster Name\"}, inplace=True)  # renaming multiple columns\n",
        "\n",
        "# merge keyword volume / CPC / Pos / URL etc data from original dataframe back in\n",
        "df_matched = pd.merge(df_matched, df_1, on=\"Keyword\", how=\"left\")\n",
        "\n",
        "# rename traffic (acs) / (desc) to 'Traffic for standardisation\n",
        "df_matched.rename(columns={\"Traffic (desc)\": \"Traffic\", \"Traffic (asc)\": \"Traffic\"}, inplace=True)\n",
        "\n",
        "if col_len > 1:\n",
        "\n",
        "    # fill in missing values\n",
        "    df_matched.fillna({\"Traffic\": 0, \"CPC\": 0}, inplace=True)\n",
        "    df_matched['Traffic'] = df_matched['Traffic'].round(0)\n",
        "    # ------------------------- group the data and merge in original stats -------------------------------------------------\n",
        "    if not adwords_check:\n",
        "        try:\n",
        "            # make dedicated grouped dataframe\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Difficulty\": \"median\", \"CPC\": \"median\", \"Traffic\": sum}).reset_index())\n",
        "        except Exception:\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Traffic\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"Difficulty\": \"Cluster KD (Median)\", \"CPC\": \"Cluster CPC (Median)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "    if adwords_check:\n",
        "\n",
        "        df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "            {\"Volume\": sum, \"CTR\": \"median\", \"Cost\": sum, \"Traffic\": sum, \"All conv. value\": sum, \"Conversions\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"CTR\": \"Cluster CTR (Median)\", \"Cost\": \"Cluster Cost (Sum)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\", \"All conv. value\": \"All conv. value (Sum)\", \"Conversions\": \"Cluster Conversions (Sum)\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "        del df_matched['To']\n",
        "        del df_matched['Similarity']\n",
        "\n",
        "    # ---------------------------- clean and sort the final output -----------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        df_matched.drop_duplicates(subset=[\"URL\", \"Keyword\"], keep=\"first\", inplace=True)  # drop if both kw & url are duped\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "h3w8Enm25Eol"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not adwords_check:\n",
        "    cols = (\n",
        "        \"Cluster Name\",\n",
        "        \"Keyword\",\n",
        "        \"Cluster Size\",\n",
        "        \"Cluster Volume\",\n",
        "        \"Cluster KD (Median)\",\n",
        "        \"Cluster CPC (Median)\",\n",
        "        \"Cluster Traffic\",\n",
        "        \"Volume\",\n",
        "        \"Difficulty\",\n",
        "        \"CPC\",\n",
        "        \"Traffic\",\n",
        "        \"URL\",\n",
        "    )\n",
        "\n",
        "    df_matched = df_matched.reindex(columns=cols)\n",
        "\n",
        "    try:\n",
        "        if gsc_data:\n",
        "            cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\", \"Cluster Volume\", \"Cluster Traffic\", \"Volume\", \"Traffic\"\n",
        "            df_matched = df_matched.reindex(columns=cols)\n",
        "    except NameError:\n",
        "        pass"
      ],
      "metadata": {
        "id": "OYdoxrxX5GsQ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ get the keyword with the highest search volume to replace the auto generated tag name with --------------\n",
        "\n",
        "if col_len > 1:\n",
        "    if parent_by_vol:\n",
        "        df_matched['vol_max'] = df_matched.groupby(['Cluster Name'])['Volume'].transform(max)\n",
        "        # this sort is mandatory for the renaming to work properly by floating highest values to the top of the cluster\n",
        "        df_matched.sort_values([\"Cluster Name\", \"Cluster Volume\", \"Volume\"], ascending=[False, True, False], inplace=True)\n",
        "        df_matched['exact_vol_match'] = df_matched['vol_max'] == df_matched['Volume']\n",
        "        df_matched.loc[df_matched['exact_vol_match'] == True, 'highest_ranked_keyword'] = df_matched['Keyword']\n",
        "        df_matched['highest_ranked_keyword'] = df_matched['highest_ranked_keyword'].fillna(method='ffill')\n",
        "        df_matched['Cluster Name'] = df_matched['highest_ranked_keyword']\n",
        "        del df_matched['vol_max']\n",
        "        del df_matched['exact_vol_match']\n",
        "        del df_matched['highest_ranked_keyword']\n",
        "if adwords_check:\n",
        "    df_matched = df_matched.rename(columns={\"Volume\": \"Impressions\", \"Traffic\": \"Clicks\", \"Cluster Traffic\": \"Cluster Clicks (Sum)\"})\n",
        "  \n",
        "# count cluster_size\n",
        "df_matched['Cluster Size'] = df_matched['Cluster Name'].map(df_matched.groupby('Cluster Name')['Cluster Name'].count())\n",
        "df_matched.loc[df_matched[\"Cluster Size\"] == 1, \"Cluster Name\"] = \"no_cluster_available\""
      ],
      "metadata": {
        "id": "KDYe9xsL5IcF"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------- final output ------------------------------------------------------------------\n",
        "# sort on cluster size\n",
        "df_matched.sort_values([\"Cluster Size\", \"Cluster Name\", \"Cluster Volume\"], ascending=[False, True, False], inplace=True)\n",
        "\n",
        "try:\n",
        "    if gsc_data:\n",
        "        df_matched.rename(\n",
        "            columns={\"Cluster Volume\": \"Cluster Impressions\", \"Cluster Traffic\": \"Cluster Clicks\", \"Traffic\": \"Clicks\",\n",
        "                     \"Volume\": \"Impressions\"}, inplace=True)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if col_len == 1:\n",
        "    cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\"\n",
        "    df_matched = df_matched.reindex(columns=cols)"
      ],
      "metadata": {
        "id": "mF4AkUTR5KKP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_matched)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWiO6pE65Lks",
        "outputId": "6d45da12-28a6-4fd0-a2a5-bbe39c24ea56"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Cluster Name                          Keyword  Cluster Size\n",
            "0     món ăn cho bé ăn cơm nát                   nấu ăn cho trẻ         220.0\n",
            "12    món ăn cho bé ăn cơm nát                  cháo hàu cho bé         220.0\n",
            "17    món ăn cho bé ăn cơm nát                      nấu cháo cá         220.0\n",
            "20    món ăn cho bé ăn cơm nát              các món cháo cho bé         220.0\n",
            "22    món ăn cho bé ăn cơm nát                    món ăn cho bé         220.0\n",
            "...                        ...                              ...           ...\n",
            "1752      no_cluster_available  các món ngon dành cho bé 3 tuổi           1.0\n",
            "244                       None                       món ăn dặm           NaN\n",
            "256                       None                   các món ăn dặm           NaN\n",
            "824                       None                 đồ nấu ăn trẻ em           NaN\n",
            "1194                      None                ghẹ bé làm món gì           NaN\n",
            "\n",
            "[1756 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - add in intent markers\n",
        "colname = df_matched.columns[1]\n",
        "df_matched.loc[df_matched[colname].str.contains(info_filter), \"Informational\"] = \"Informational\"\n",
        "df_matched.loc[df_matched[colname].str.contains(comm_invest_filter), \"Commercial Investigation\"] = \"Commercial Investigation\"\n",
        "df_matched.loc[df_matched[colname].str.contains(trans_filter), \"Transactional\"] = \"Transactional\""
      ],
      "metadata": {
        "id": "WK-lVZZO5Ppl"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_matched.to_csv('your_keywords_clustered.csv', index=False)\n",
        "files.download(\"your_keywords_clustered.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "-q367B3nc77J",
        "outputId": "4192632b-5820-4822-d709-ca950d1996fc"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9bbd41a9-0847-4eb0-8ab8-1e1d32d4ef9a\", \"your_keywords_clustered.csv\", 126422)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}