{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AncalagonDBlack/source-code-seo-tool/blob/master/Extracting_Backlinks_from_GSC_Ch%C6%B0%C6%A1ng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1oTwGuqpG3"
      },
      "source": [
        "https://tealfeed.com/methods-extracting-backlinks-google-search-console-cqwwm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PdNL9NkoH55"
      },
      "source": [
        "https://medium.com/@liviuoltean/seo-how-to-scrape-googles-search-console-for-backlinks-data-3893503cf128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfA5hrqsminG"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests,time\n",
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "from google.colab import files\n",
        "input=\"yourdomain\" #nhập domain zô\n",
        "headers = {\n",
        "    \"authority\": \"search.google.com\",\n",
        "    \"method\":\"GET\",\n",
        "    \"path\":f\"/search-console/links?resource_id=https%3A%2F%2F{input}%2F\",\n",
        "    \"scheme\":\"https\",  \n",
        "    \"accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "    \"accept-encoding\":\"gzip, deflate, br\",\n",
        "    \"accept-language\":\"en-GB,en-US;q=0.9,en;q=0.8,ro;q=0.7\",\n",
        "    \"cache-control\":\"no-cache\",\n",
        "    \"pragma\":\"no-cache\",\n",
        "    \"sec-ch-ua\":\"navigate\",\n",
        "    \"sec-fetch-site\":\"same-origin\",\n",
        "    \"sec-fetch-user\":\"?1\",\n",
        "    \"upgrade-insecure-requests\":\"1\",\n",
        "    \"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36\",\n",
        "    \"x-client-data\":\"CJG2yQEIpLbJAQjBtskBCKmdygEI94LLAQiUocsBCLi8zAEI6rzMAQjE4cwBCMXhzAEI2OjMAQj/6MwBCPPqzAEInezMAQjm8cwB\",\n",
        "    \"sec-ch-ua\":'\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"100\", \"Google Chrome\";v=\"100\"',\n",
        "    \"sec-ch-ua-mobile\": \"?0\",\n",
        "    \"sec-fetch-dest\": \"document\",\n",
        "    \"sec-fetch-mode\": \"navigate\"\n",
        "}\n",
        "cookies = {\n",
        "    \"__Secure-1PAPISID\":\"info\", #:\"[your-info]\", cập nhật cái này\n",
        "    \"__Secure-1PSID\":\"\",#:\"[your-info]\",\n",
        "    \"__Secure-3PAPISID\":\"\",#:\"[your-info]\",\n",
        "    \"__Secure-3PSID\":\"\",#:\"[your-info]\",\n",
        "    \"__Secure-3PSIDCC\":\"\",#:\"[your-info]\", cập nhật cái này\n",
        "    \"1P_JAR\":\"2022-12-06-02\",#:\"[your-info]\",\n",
        "    \"NID\":\"\", #:\"[your-info]\",\n",
        "    \"APISID\":\"\",#:\"[your-info]\",\n",
        "    #\"CONSENT\":\"[your-info]\",\n",
        "    \"HSID\":\"\",#:\"[your-info]\",\n",
        "    \"SAPISID\":\"\",#:\"[your-info]\",\n",
        "    \"SID\":\"\",#:\"[your-info]\",\n",
        "    \"SIDCC\":\"\",#:\"[your-info]\", cập nhật cái này\n",
        "    \"SSID\":\"\",#:\"[your-info]\",\n",
        "    \"_ga\":\"\",#:\"[your-info]\",\n",
        "    #\"OGPC\":\"[your-info]\",\n",
        "    \"OTZ\":\"\"#:\"[your-info]\", ít đổi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04nGKDk4pSfV"
      },
      "outputs": [],
      "source": [
        "genericURL = f\"https://search.google.com/search-console/links/drilldown?resource_id=https%3A%2F%2F{input}%2F&type=DOMAIN&target=&domain=\"\n",
        "req = requests.get(genericURL, headers=headers, cookies=cookies)\n",
        "soup = BeautifulSoup(req.content, 'html.parser')\n",
        "print(soup) #check soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I08sxGglpWZE"
      },
      "outputs": [],
      "source": [
        "regex = re.compile('nJ0sOc wNFy3d ptEsvc*')\n",
        "g_data = soup.findAll(\"tr\", {\"class\": regex})\n",
        "dfdomains = pd.DataFrame()\n",
        "print(g_data)\n",
        "countchovui=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6gexBC615QJ"
      },
      "outputs": [],
      "source": [
        "for externalDomain in g_data:\n",
        "    #countchovui=countchovui+1\n",
        "    #if countchovui<311:\n",
        "    #  continue\n",
        "    out = re.search(r'<div class=\"OOHai\">(.*?(?=<))', str(externalDomain))\n",
        "    rawdata=externalDomain.findAll(\"span\", {\"class\": \"PkjLuf\"})\n",
        "    URLList=out.group(1)\n",
        "    print(URLList)\n",
        "    urllink=rawdata[1].text\n",
        "    urldest=rawdata[2].text\n",
        "    url = f\"https://search.google.com/search-console/links/drilldown?resource_id=https%3A%2F%2F{input}%2F&type=DOMAIN&target=&domain={URLList}\"\n",
        "    time.sleep(5)\n",
        "    request = requests.get(url, headers=headers, cookies=cookies)\n",
        "    soup_1 = BeautifulSoup(request.content, 'html.parser')\n",
        "    count_step1=0\n",
        "    for row in soup_1.findAll(\"tr\", {\"class\": regex}):\n",
        "      count_step1=count_step1+1\n",
        "      if count_step1>100:\n",
        "        break\n",
        "      rawdata=row.findAll(\"span\", {\"class\": \"PkjLuf\"})\n",
        "      output = rawdata[0].text\n",
        "      stripped_output = output.replace(\"\", \"\")\n",
        "      urllink_1=rawdata[1].text\n",
        "      url_secondary = f\"https://search.google.com/search-console/links/drilldown?resource_id=https%3A%2F%2F{input}%2F&type=DOMAIN&target={stripped_output}&domain={URLList}\"\n",
        "      time.sleep(3)\n",
        "      request_secondary = requests.get(url_secondary, headers=headers, cookies=cookies)\n",
        "      soup_secondary = BeautifulSoup(request_secondary.content, 'html.parser')\n",
        "      count_step2=0\n",
        "      for row_1 in soup_secondary.findAll(\"tr\", {\"class\": regex}):\n",
        "        count_step2=count_step2+1\n",
        "        if count_step2>50:\n",
        "          break\n",
        "        rawdata2=row_1.findAll(\"span\", {\"class\": \"PkjLuf\"})\n",
        "        output_last = rawdata2[0].text.replace(\"\", \"\")\n",
        "        urldest_2=rawdata2[1].text.replace(\"\", \"\")\n",
        "        dfdomains = dfdomains.append(pd.DataFrame({\"Trang web\":[URLList],\"Số trang liên kết\":[urllink],\"Số trang đích\":[urldest],\"Trang đích\":[stripped_output],\"Liên kết bên ngoài\":[urllink_1],\"Trang liên kết\":[output_last],\"URL đích (nếu khác nhau)\":[urldest_2]}))\n",
        "      #dfdomains = dfdomains.append(pd.DataFrame({\"Trang web\":\"\",\"Số trang liên kết\":\"\",\"Số trang đích\":\"\",\"Trang đích\":[stripped_output],\"Liên kết bên ngoài\":[urllink_1],}))\n",
        "    #dfdomains = dfdomains.append(pd.DataFrame({\"Trang web\":[URLList],\"Số trang liên kết\":[urllink],\"Số trang đích\":[urldest]}))\n",
        "#domainsList = dfdomains[\"Trang web\"].to_list()\n",
        "#print(dfdomains) #check list domain xem suất đủ ko "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfdomains.to_csv('Backlink.csv', index=False)\n",
        "files.download('Backlink.csv')"
      ],
      "metadata": {
        "id": "_H9Y6t1WN3H3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}